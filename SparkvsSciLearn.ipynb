{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b1a9e40",
   "metadata": {},
   "source": [
    "# Spark vs SciKit Learn: A Comparative Analysis\n",
    "\n",
    "This document provides a comparative analysis of machine learning models using Spark and SciKit Learn. The focus is on building, training, and evaluating three different classifiers: Decision Tree Classifier, Naive Bayes, and Random Forest.\n",
    "\n",
    "## Models to be Compared:\n",
    "\n",
    " - Decision Tree Classifier\n",
    " - Naive Bayes\n",
    " - Random Forest\n",
    "\n",
    "## A quick summary:\n",
    "\n",
    "- Import Libraries\n",
    "- Build Spark Session\n",
    "- Data Load\n",
    "- Data Exploration & Preparation\n",
    "- Feature Engineering\n",
    "- Data Scaling\n",
    "- Data Split\n",
    "- Build, Train & Evaluate Model\n",
    "- Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89da2f47",
   "metadata": {},
   "source": [
    "Import required libraries for Spark and SciKit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7bddc54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Generic Libraries -----\n",
    "\n",
    "import numpy as np # Linear algebra\n",
    "import pandas as pd # Data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# ----- Pyspark Libraries -----\n",
    "\n",
    "# Spark base libraries\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark machine learning classifier libraries\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier, NaiveBayes\n",
    "\n",
    "# Spark evaluation libraries\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Spark feature transformation libraries\n",
    "from pyspark.ml.feature import StandardScaler, StringIndexer\n",
    "\n",
    "# Spark pipeline libraries\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Spark DenseVector libraries\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "# ----- SciKit Learn Libraries -----\n",
    "\n",
    "# Data preprocessing libraries\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Machine learning classifier libraries\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Tablating Data\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Garbage collection\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929e11ad",
   "metadata": {},
   "source": [
    "# Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7b78bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/08/05 23:14:44 WARN Utils: Your hostname, Harrys-MacBook-Pro.local, resolves to a loopback address: 127.0.0.1; using 192.168.0.64 instead (on interface en0)\n",
      "25/08/05 23:14:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/05 23:14:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# build a Spark session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Spark SciKit\")\n",
    "    .config(\"spark.driver.bindAddress\", \"localhost\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e283ec71",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2423931b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.0.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316bfc13",
   "metadata": {},
   "source": [
    "# Data Load\n",
    "Load data into the two dataframes: `df_spark` for Spark and `df_sk` for SciKit Learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0cfbb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_csv = 'data/iris.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16d82c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[sepal_length: double, sepal_width: double, petal_length: double, petal_width: double, species: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PySpark DataFrame\n",
    "df_spark = spark.read.csv(iris_csv, header=True, inferSchema=True)\n",
    "df_spark.cache() # For fast reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35a82321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKLearn DataFrame\n",
    "df_sk = pd.read_csv(iris_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dc0d6d",
   "metadata": {},
   "source": [
    "# Data Exploration & Preparation\n",
    "Explore the data in both Spark and SciKit Learn dataframes. This includes checking for null values, data types, and basic statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1152c821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark -  150\n",
      "SciKit Learn -  (150, 5)\n"
     ]
    }
   ],
   "source": [
    "# Total Count\n",
    "print(\"PySpark - \", df_spark.count())\n",
    "print(\"SciKit Learn - \", df_sk.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0010ea7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sepal_length: double (nullable = true)\n",
      " |-- sepal_width: double (nullable = true)\n",
      " |-- petal_length: double (nullable = true)\n",
      " |-- petal_width: double (nullable = true)\n",
      " |-- species: string (nullable = true)\n",
      "\n",
      "PySpark -  None\n"
     ]
    }
   ],
   "source": [
    "# Data Type\n",
    "print(\"PySpark - \", df_spark.printSchema())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd8b21c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   sepal_length  150 non-null    float64\n",
      " 1   sepal_width   150 non-null    float64\n",
      " 2   petal_length  150 non-null    float64\n",
      " 3   petal_width   150 non-null    float64\n",
      " 4   species       150 non-null    object \n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 6.0+ KB\n",
      "SciKit Learn -  None\n"
     ]
    }
   ],
   "source": [
    "print(\"SciKit Learn - \", df_sk.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da92f6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|species|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "|         5.1|        3.5|         1.4|        0.2| setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2| setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2| setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2| setosa|\n",
      "|         5.0|        3.6|         1.4|        0.2| setosa|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "PySpark -  None\n",
      "SciKit Learn - \n",
      "     sepal_length  sepal_width  petal_length  petal_width species\n",
      "0           5.1          3.5           1.4          0.2  setosa\n",
      "1           4.9          3.0           1.4          0.2  setosa\n",
      "2           4.7          3.2           1.3          0.2  setosa\n",
      "3           4.6          3.1           1.5          0.2  setosa\n",
      "4           5.0          3.6           1.4          0.2  setosa\n"
     ]
    }
   ],
   "source": [
    "# Display Records\n",
    "print(\"PySpark - \", df_spark.show(5))\n",
    "print(\"SciKit Learn - \\n \", df_sk.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b03a7f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|   species|count|\n",
      "+----------+-----+\n",
      "| virginica|   50|\n",
      "|versicolor|   50|\n",
      "|    setosa|   50|\n",
      "+----------+-----+\n",
      "\n",
      "PySpark -  None\n",
      "SciKit Learn - \n",
      "  species\n",
      "setosa        50\n",
      "versicolor    50\n",
      "virginica     50\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Record per species\n",
    "print(\"PySpark - \", df_spark.groupBy(\"species\").count().show())\n",
    "print(\"SciKit Learn - \\n \", df_sk.groupby('species').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36bbc1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/05 23:14:51 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------------+------------------+------------------+---------+\n",
      "|summary|      sepal_length|        sepal_width|      petal_length|       petal_width|  species|\n",
      "+-------+------------------+-------------------+------------------+------------------+---------+\n",
      "|  count|               150|                150|               150|               150|      150|\n",
      "|   mean| 5.843333333333335| 3.0540000000000007|3.7586666666666693|1.1986666666666672|     NULL|\n",
      "| stddev|0.8280661279778637|0.43359431136217375| 1.764420419952262|0.7631607417008414|     NULL|\n",
      "|    min|               4.3|                2.0|               1.0|               0.1|   setosa|\n",
      "|    max|               7.9|                4.4|               6.9|               2.5|virginica|\n",
      "+-------+------------------+-------------------+------------------+------------------+---------+\n",
      "\n",
      "PySpark -  None\n",
      "SciKit Learn - \n",
      "                count      mean       std  min  25%   50%  75%  max\n",
      "sepal_length  150.0  5.843333  0.828066  4.3  5.1  5.80  6.4  7.9\n",
      "sepal_width   150.0  3.054000  0.433594  2.0  2.8  3.00  3.3  4.4\n",
      "petal_length  150.0  3.758667  1.764420  1.0  1.6  4.35  5.1  6.9\n",
      "petal_width   150.0  1.198667  0.763161  0.1  0.3  1.30  1.8  2.5\n"
     ]
    }
   ],
   "source": [
    "# Summary Statistics\n",
    "print(\"PySpark - \", df_spark.describe().show())\n",
    "print(\"SciKit Learn - \\n \", df_sk.describe().transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85f21fd",
   "metadata": {},
   "source": [
    "In order for our model to make predictions the Species (Label colum) should be a numerical value. \n",
    "\n",
    "To achieve this, apply String Indexing on the Species columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ad45383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark\n",
    "spark_string_indexer = StringIndexer(inputCol=\"species\", outputCol=\"species_index\")\n",
    "df_spark = spark_string_indexer.fit(df_spark).transform(df_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aef1c9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKLearn\n",
    "label_encoder = LabelEncoder()\n",
    "df_sk['species_index'] = label_encoder.fit_transform(df_sk['species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "802f3951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+-------------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|species|species_index|\n",
      "+------------+-----------+------------+-----------+-------+-------------+\n",
      "|         5.1|        3.5|         1.4|        0.2| setosa|          0.0|\n",
      "|         4.9|        3.0|         1.4|        0.2| setosa|          0.0|\n",
      "|         4.7|        3.2|         1.3|        0.2| setosa|          0.0|\n",
      "|         4.6|        3.1|         1.5|        0.2| setosa|          0.0|\n",
      "|         5.0|        3.6|         1.4|        0.2| setosa|          0.0|\n",
      "+------------+-----------+------------+-----------+-------+-------------+\n",
      "only showing top 5 rows\n",
      "PySpark -  None\n",
      "SciKit Learn - \n",
      "     sepal_length  sepal_width  petal_length  petal_width species  species_index\n",
      "0           5.1          3.5           1.4          0.2  setosa              0\n",
      "1           4.9          3.0           1.4          0.2  setosa              0\n",
      "2           4.7          3.2           1.3          0.2  setosa              0\n",
      "3           4.6          3.1           1.5          0.2  setosa              0\n",
      "4           5.0          3.6           1.4          0.2  setosa              0\n"
     ]
    }
   ],
   "source": [
    "# Inspect the DataFrame after String Indexing\n",
    "print(\"PySpark - \", df_spark.show(5))\n",
    "print(\"SciKit Learn - \\n \", df_sk.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a4c1e8",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "The spark model needs two columns: \"label\" and \"features\". The \"label\" column is the target variable, and the \"features\" column contains the input features.\n",
    "\n",
    "Create a separate dataframe `df_spark_features` that contains the \"label\" and \"features\" columns. Then define the features using `DenseVector`.\n",
    "\n",
    "Dense Vector is a local vector backed by a double array representing its entry values. It is used to represent the features in Spark MLlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6a68ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+-----------+------------+-----------+\n",
      "|species_index|sepal_length|sepal_width|petal_length|petal_width|\n",
      "+-------------+------------+-----------+------------+-----------+\n",
      "|          0.0|         5.1|        3.5|         1.4|        0.2|\n",
      "|          0.0|         4.9|        3.0|         1.4|        0.2|\n",
      "|          0.0|         4.7|        3.2|         1.3|        0.2|\n",
      "|          0.0|         4.6|        3.1|         1.5|        0.2|\n",
      "|          0.0|         5.0|        3.6|         1.4|        0.2|\n",
      "+-------------+------------+-----------+------------+-----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Create a df with reordered columns\n",
    "df_spark_features = df_spark.select(\"species_index\", \"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\")\n",
    "\n",
    "df_spark_features.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c59b8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features using DenseVector\n",
    "input_data = df_spark_features.rdd.map(lambda row: (row[0], DenseVector(row[1:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c283526a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_spark_index = spark.createDataFrame(input_data, [\"label\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97390e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|label|         features|\n",
      "+-----+-----------------+\n",
      "|  0.0|[5.1,3.5,1.4,0.2]|\n",
      "|  0.0|[4.9,3.0,1.4,0.2]|\n",
      "|  0.0|[4.7,3.2,1.3,0.2]|\n",
      "|  0.0|[4.6,3.1,1.5,0.2]|\n",
      "|  0.0|[5.0,3.6,1.4,0.2]|\n",
      "+-----+-----------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df_spark_index.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853b7ec3",
   "metadata": {},
   "source": [
    "Feature and target selection using SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa64dcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_features = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n",
    "target = ['species']\n",
    "\n",
    "X = df_sk[sk_features]\n",
    "y = df_sk[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c527233",
   "metadata": {},
   "source": [
    "# Scaling the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8316d7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark\n",
    "spark_standard_scaler = pyspark.ml.feature.StandardScaler(\n",
    "    inputCol=\"features\", outputCol=\"features_scaled\"\n",
    ")\n",
    "spark_scaler = spark_standard_scaler.fit(df_spark_index)\n",
    "df_spark_scaled = spark_scaler.transform(df_spark_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4baa9236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKLearn DataFrame\n",
    "sk_standard_scaler = StandardScaler()\n",
    "df_sk_scaled = sk_standard_scaler.fit_transform(X)\n",
    "df_sk_scaled = pd.DataFrame(\n",
    "    df_sk_scaled, columns=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9200b6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+--------------------+\n",
      "|label|         features|     features_scaled|\n",
      "+-----+-----------------+--------------------+\n",
      "|  0.0|[5.1,3.5,1.4,0.2]|[6.15892840883878...|\n",
      "|  0.0|[4.9,3.0,1.4,0.2]|[5.9174018045706,...|\n",
      "|  0.0|[4.7,3.2,1.3,0.2]|[5.67587520030241...|\n",
      "|  0.0|[4.6,3.1,1.5,0.2]|[5.55511189816831...|\n",
      "|  0.0|[5.0,3.6,1.4,0.2]|[6.03816510670469...|\n",
      "+-----+-----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "None\n",
      "   sepal_length  sepal_width  petal_length  petal_width\n",
      "0     -0.900681     1.032057     -1.341272    -1.312977\n",
      "1     -1.143017    -0.124958     -1.341272    -1.312977\n",
      "2     -1.385353     0.337848     -1.398138    -1.312977\n",
      "3     -1.506521     0.106445     -1.284407    -1.312977\n",
      "4     -1.021849     1.263460     -1.341272    -1.312977\n"
     ]
    }
   ],
   "source": [
    "# Inspect the Scaled Data\n",
    "print(df_spark_scaled.show(5))\n",
    "print(df_sk_scaled.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5d2c912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the Features column\n",
    "df_spark_scaled = df_spark_scaled.drop(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d31c9bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|     features_scaled|\n",
      "+-----+--------------------+\n",
      "|  0.0|[6.15892840883878...|\n",
      "|  0.0|[5.9174018045706,...|\n",
      "|  0.0|[5.67587520030241...|\n",
      "|  0.0|[5.55511189816831...|\n",
      "|  0.0|[6.03816510670469...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df_spark_scaled.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9d7967",
   "metadata": {},
   "source": [
    "# Data Split\n",
    "Split the data into training and testing sets for both Spark and SciKit Learn dataframes.\n",
    "Using train : test split of 90:10 for both Spark and SciKit Learn dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6af3452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark\n",
    "spark_train, spark_test = df_spark_scaled.randomSplit([0.9, 0.1], seed=42)\n",
    "\n",
    "# SKLearn\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_sk_scaled, df_sk['species_index'], test_size=0.1, random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "144a6bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|     features_scaled|\n",
      "+-----+--------------------+\n",
      "|  0.0|[5.19282199176603...|\n",
      "|  0.0|[5.31358529390013...|\n",
      "|  0.0|[5.31358529390013...|\n",
      "|  0.0|[5.31358529390013...|\n",
      "|  0.0|[5.43434859603422...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "PySpark -  None\n",
      "SKLearn - \n",
      "       sepal_length  sepal_width  petal_length  petal_width\n",
      "97       0.432165    -0.356361      0.307833     0.133226\n",
      "15      -0.173674     3.114684     -1.284407    -1.050031\n",
      "12      -1.264185    -0.124958     -1.341272    -1.444450\n",
      "114     -0.052506    -0.587764      0.762759     1.579429\n",
      "100      0.553333     0.569251      1.274550     1.710902\n"
     ]
    }
   ],
   "source": [
    "# Inspect the Training Data\n",
    "print(\"PySpark - \", spark_train.show(5))\n",
    "print(\"SKLearn - \\n \", X_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd587cc",
   "metadata": {},
   "source": [
    "# Build, Train & Evaluate Model\n",
    "Build, train, and evaluate the three classifiers: Decision Tree Classifier, Naive Bayes, and Random Forest.\n",
    "\n",
    "Then compaire their accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1a70e25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ['Decision Tree', 'Random Forest', 'Naive Bayes']\n",
    "model_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93437dd",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "27bec23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark\n",
    "dtc_spark = pyspark.ml.classification.DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features_scaled\")\n",
    "dtc_spark_model = dtc_spark.fit(spark_train)\n",
    "dtc_spark_predictions = dtc_spark_model.transform(spark_test)\n",
    "\n",
    "#Evaluate the model\n",
    "dtc_spark_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "dtc_spark_accuracy = dtc_spark_evaluator.evaluate(dtc_spark_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b8b06bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier Accuracy (PySpark): 0.7778\n"
     ]
    }
   ],
   "source": [
    "print(f\"Decision Tree Classifier Accuracy (PySpark): {dtc_spark_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "40e0386d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier Accuracy (SKLearn): 0.8667\n"
     ]
    }
   ],
   "source": [
    "# SKLearn\n",
    "dtc_sk = DecisionTreeClassifier(random_state=43)\n",
    "dtc_sk.fit(X_train, y_train)\n",
    "dtc_sk_predictions = dtc_sk.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "dtc_sk_accuracy = accuracy_score(y_test, dtc_sk_predictions)\n",
    "print(f\"Decision Tree Classifier Accuracy (SKLearn): {dtc_sk_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8644ea1f",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c858e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier Accuracy (PySpark): 0.7778\n"
     ]
    }
   ],
   "source": [
    "# Spark\n",
    "rfc_spark = pyspark.ml.classification.RandomForestClassifier(labelCol=\"label\", featuresCol=\"features_scaled\", numTrees=100)\n",
    "rfc_spark_model = rfc_spark.fit(spark_train)\n",
    "rfc_spark_predictions = rfc_spark_model.transform(spark_test)\n",
    "\n",
    "# Evaluate the model\n",
    "rfc_spark_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "rfc_spark_accuracy = rfc_spark_evaluator.evaluate(rfc_spark_predictions)\n",
    "\n",
    "print(f\"Random Forest Classifier Accuracy (PySpark): {rfc_spark_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "036ba7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_sk = RandomForestClassifier(n_estimators=1000, criterion='entropy', random_state=None, bootstrap=True)\n",
    "rfc_sk.fit(X_train, y_train)\n",
    "rfc_sk_predictions = rfc_sk.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "rfc_sk_accuracy = accuracy_score(y_test, rfc_sk_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "49c39283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier Accuracy (SKLearn): 0.9333\n"
     ]
    }
   ],
   "source": [
    "print(f\"Random Forest Classifier Accuracy (SKLearn): {rfc_sk_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfae6be",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e8628105",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Classifier Accuracy (PySpark): 0.8889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/05 23:42:35 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n"
     ]
    }
   ],
   "source": [
    "# Spark\n",
    "nbc_spark = pyspark.ml.classification.NaiveBayes(smoothing=1.0, modelType=\"gaussian\", labelCol=\"label\", featuresCol=\"features_scaled\")\n",
    "nbc_spark_model = nbc_spark.fit(spark_train)\n",
    "nbc_spark_predictions = nbc_spark_model.transform(spark_test)\n",
    "\n",
    "# Evaluate the model\n",
    "nbc_spark_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "nbc_spark_accuracy = nbc_spark_evaluator.evaluate(nbc_spark_predictions)\n",
    "print(f\"Naive Bayes Classifier Accuracy (PySpark): {nbc_spark_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c71ae159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Classifier Accuracy (SKLearn): 1.0000\n"
     ]
    }
   ],
   "source": [
    "# SKLearn\n",
    "nbc_sk = GaussianNB()\n",
    "nbc_sk.fit(X_train, y_train)\n",
    "nbc_sk_predictions = nbc_sk.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "nbc_sk_accuracy = accuracy_score(y_test, nbc_sk_predictions)\n",
    "print(f\"Naive Bayes Classifier Accuracy (SKLearn): {nbc_sk_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c356a43f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1376"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Free up memory\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfabf21c",
   "metadata": {},
   "source": [
    "# Model Comparison\n",
    "\n",
    "Finally, compare the accuracy of the models built using Spark and SciKit Learn. The comparison will be based on the accuracy scores obtained from the evaluation of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "47705c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = [[\"Decision Tree Classifier\", \"{:.2%}\".format(dtc_spark_accuracy), '{:.2%}'.format(dtc_sk_accuracy)], \\\n",
    "              [\"Random Forest Classifier\", \"{:.2%}\".format(rfc_spark_accuracy), '{:.2%}'.format(rfc_sk_accuracy)], \\\n",
    "              [\"Naive Bayes Classifier\", \"{:.2%}\".format(nbc_spark_accuracy), '{:.2%}'.format(nbc_sk_accuracy)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3ca90b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Model          PySpark Accuracy    SKLearn Accuracy\n",
      "------------------------  ------------------  ------------------\n",
      "Decision Tree Classifier  77.78%              86.67%\n",
      "Random Forest Classifier  77.78%              93.33%\n",
      "Naive Bayes Classifier    88.89%              100.00%\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(model_data, headers=[\"Classifier Model\", \"PySpark Accuracy\", \"SKLearn Accuracy\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eec7ed9",
   "metadata": {},
   "source": [
    "Conclusion - The SciKit Classifier Models perform slightly better than the PySpark Models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f234372",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
